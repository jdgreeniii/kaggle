{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annoying-thing",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gan_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-44ce3a97ae98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgan_models\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgan_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_dataset_from_folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gan_models'"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "from gan_models import generator, discriminator \n",
    "from gan_utils import create_dataset_from_folder\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latent_dim, z): # paper -> no fully connected layers, 'batchnorm in both the generator and the discriminator'\n",
    "    x = Sequential()\n",
    "    x.add(layers.Dense(128 * 16 * 16, input_dim = latent_dim))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    x.add(layers.Reshape((16,16,128)))\n",
    "    \n",
    "    x.add(layers.Conv2D(256, 5 ,padding='same'))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    \n",
    "    x.add(layers.Conv2DTranspose(256, 4, strides=2, padding='same'))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    \n",
    "    x.add(layers.Conv2D(256, 5, padding='same'))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    x.add(layers.Conv2DTranspose(256, 4, strides=3, padding='same'))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    x.add(layers.Conv2D(256, 5, padding='same'))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    \n",
    "    x.add(layers.Conv2D(z, 7, activation='relu', padding='same'))\n",
    "    return x\n",
    "\n",
    "\n",
    "def discriminator(height, width, z): # paper -> Use LeakyReLU activation in the discriminator for all layers\n",
    "    x = Sequential() #  In the LeakyReLU, the slope of the leak was set to 0.2 in all models\n",
    "    x.add(layers.Conv2D(128,(3,3),input_shape=(height, width, z)))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    \n",
    "    x.add(layers.Conv2D(256,4,strides=2))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    x.add(layers.Conv2D(256,4,strides=2))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    x.add(layers.Conv2D(128,4,strides=2))\n",
    "    x.add(layers.LeakyReLU())\n",
    "    \n",
    "    x.add(layers.Flatten())\n",
    "    x.add(layers.Dropout(.4))\n",
    "    x.add(layers.Dense(512))\n",
    "    x.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    latent_dim = 32\n",
    "    height = 96\n",
    "    width = 96\n",
    "    z = 96\n",
    "    norm = 686 # No pre-processing was applied to training images besides scaling to the range of the tanh activation function [-1, 1].\n",
    "    data_dir = './data/model_input/'\n",
    "    save_dir = 'model/houses/'\n",
    "    save_every = 10\n",
    "\n",
    "    gen = generator(latent_dim, z)\n",
    "    dis = discriminator(height, width, z)\n",
    "\n",
    "    print('GENERATOR SUMMARY')\n",
    "    print(gen.summary())\n",
    "    print('DISCRIMINATOR SUMMARY')\n",
    "    print(dis.summary())\n",
    "\n",
    "    dis.compile(optimizer=Adam(lr=.00001), loss='binary_crossentropy')\n",
    "\n",
    "    gan_input = tensorflow.keras.Input(shape=(latent_dim,))\n",
    "    gan_output = dis(gen(gan_input))\n",
    "    gan = tensorflow.keras.models.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer=Adam(lr=.00001, clipvalue=1, decay=1e-8), loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "    x_train = create_dataset_from_folder(data_dir)\n",
    "    print('DATASET SHAPE')\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    x_train = x_train.reshape((x_train.shape[0],) + (height, width, z)).astype('float32') / norm\n",
    "\n",
    "\n",
    "    batch_size = 32\n",
    "    save_dir = 'model/houses/'\n",
    "    epochs = 500\n",
    "    num_samples_trained = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        d_avg_loss = 0\n",
    "        a_avg_loss = 0\n",
    "        print(f'Epoch: {epoch}')\n",
    "        for index in range(x_train.shape[0] // batch_size):\n",
    "            random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "            real_images = x_train[index * batch_size: (index + 1) * batch_size]\n",
    "            generated_images = gen.predict(random_latent_vectors)\n",
    "            # print(generated_images[0])\n",
    "            # sys.exit()\n",
    "            #find a way to stitch and check 3d arrays, gonna be tough with 96z axis\n",
    "\n",
    "            combined_images = np.concatenate([generated_images, real_images])\n",
    "            labels = np.concatenate([np.ones((batch_size,1)),\n",
    "                                np.zeros((batch_size, 1))])\n",
    "            #labels += .05 * np.random.random(labels.shape)\n",
    "            d_loss = dis.train_on_batch(combined_images, labels)\n",
    "            d_avg_loss += d_loss\n",
    "            random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "            misleading_targets = np.zeros((batch_size,1))\n",
    "        \n",
    "            dis.trainable = False\n",
    "            a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "            a_avg_loss += a_loss\n",
    "            dis.trainable = True\n",
    "\n",
    "            num_samples_trained += batch_size\n",
    "            \n",
    "            if epoch % save_every == 0:\n",
    "                e_save_dir = save_dir + f'{epoch}/'\n",
    "                # add unique folder for each model run, just in case\n",
    "\n",
    "                if os.path.exists(e_save_dir) and index == 0:\n",
    "                    shutil.rmtree(e_save_dir)\n",
    "                    os.makedirs(e_save_dir)\n",
    "                    os.makedirs(e_save_dir + 'generated\\\\')\n",
    "                    os.makedirs(e_save_dir + 'real\\\\')\n",
    "                elif not os.path.exists(e_save_dir) and index == 0:\n",
    "                    os.makedirs(e_save_dir)\n",
    "                    os.makedirs(e_save_dir + 'generated\\\\')\n",
    "                    os.makedirs(e_save_dir + 'real\\\\')\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "                np.save(e_save_dir + 'generated\\\\' + 'generated_house' + str(index), generated_images[0] * norm)\n",
    "                # img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "                # img.save(os.path.join(save_dir,'generated\\\\' + 'generated_frog' + str(step) + '.png'))\n",
    "                np.save(e_save_dir + 'real\\\\' + 'real_house' +str(index), real_images[0] * norm)\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            print('discriminator loss:', d_avg_loss / 82)\n",
    "            print('adversarial loss:', a_avg_loss / 82)\n",
    "            gan.save('model/models/gan.h5')\n",
    "            gen.save('model/models/gen.h5')\n",
    "            print(num_samples_trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
